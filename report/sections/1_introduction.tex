\section{Introduction}

The performance of machine learning models relies heavily on hyperparameters—configuration variables like learning rate and batch size that control the training process. Identifying the optimal configuration is a significant bottleneck in model development due to the high computational cost of evaluation. To address this complexity, we frame Hyperparameter Optimization (HPO) as a software verification problem. In this context, the model functions as the “software under test,” where a suboptimal configuration is treated as a “defect” that causes the system to violate its performance specifications (e.g., high loss or instability). HPO therefore acts as an automated test driver, searching the configuration space to verify model performance against defined quality criteria.

\subsection{Evaluation Criteria}

We evaluate the optimizers across 3 quality attributes:

\begin{itemize}
    \item \textbf{Effectiveness}: The quality of the final solution found (i.e., the best fitness score achieved).
    \item \textbf{Convergence}: The rate at which the algorithm improves its best-found solution over the course of the fixed evaluation budget.
    \item \textbf{Stability}: The consistency and reliability of the algorithm's performance across multiple independent runs (measured by variance).
\end{itemize}

\subsection{Research Questions}

This report seeks to answer the following research questions:

\begin{itemize}
    \item \textbf{RQ1}: How do representative metaheuristic algorithms compare against a randomized search baseline in terms of effectiveness and convergence rates given a fixed evaluation budget?
    \item \textbf{RQ2}: What is the difference in performance stability between the selected metaheuristic algorithms and the randomized search baseline?
\end{itemize}
