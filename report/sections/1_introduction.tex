\section{Introduction}

The performance of machine learning models often depends on their hyperparametersâ€”high-level configuration variables like learning rate or batch size that control the training process. Finding the optimal set of these configurations, or Hyperparameter Optimization (HPO), is a significant bottleneck in model development.

HPO can be framed as a software verification problem. In this context, the model is the software under test and a "defect" is a suboptimal hyperparameter configuration that causes the model to fail its performance specifications, such as by exhibiting high loss, poor generalization, or unstable training. HPO thus functions as an automated test driver, searching the configuration space to find a set of hyperparameters that verifies the model's performance against a pre-defined quality specification.

\subsection{Evaluation Criteria}

We evaluate the optimizers across 3 quality attributes:

\begin{itemize}
    \item \textbf{Effectiveness}: The quality of the final solution found (i.e., the best fitness score achieved).
    \item \textbf{Convergence}: The rate at which the algorithm improves its best-found solution over the course of the fixed evaluation budget.
    \item \textbf{Stability}: The consistency and reliability of the algorithm's performance across multiple independent runs (measured by variance).
\end{itemize}

\subsection{Research Questions}

This report seeks to answer the following research questions:

\begin{itemize}
    \item \textbf{RQ1}: How do representative metaheuristic algorithms compare against a randomized search baseline in terms of effectiveness and convergence rates given a fixed evaluation budget?
    \item \textbf{RQ2}: What is the difference in performance stability between the selected metaheuristic algorithms and the randomized search baseline?
\end{itemize}
