{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90a756a",
   "metadata": {},
   "source": [
    "# Quick Sample Flow - All Models Demo\n",
    "\n",
    "This section demonstrates a quick sample run of all implemented model classes with minimal data and iterations to verify everything works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8d72b",
   "metadata": {},
   "source": [
    "## Quick Setup - Load Minimal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63bd092a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick sample flow setup completed!\n",
      "Dataset directory: .\\..\\.cache\\processed_datasets\n",
      "CIFAR-10 path: .\\..\\.cache\\processed_datasets\\cifar10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "sys.path.append(os.path.join(os.path.curdir, \"..\"))\n",
    "from framework.data_utils import load_cifar10_data\n",
    "from models.cnn import TrainingConfig\n",
    "\n",
    "# Add parent directory to path to import our models\n",
    "sys.path.append(os.path.join(os.path.curdir, \"..\"))\n",
    "\n",
    "# Dataset directories\n",
    "DATASET_DIR = os.path.join(os.path.curdir, \"..\", \".cache\", \"processed_datasets\")\n",
    "CIFAR10_PATH = os.path.join(DATASET_DIR, \"cifar10\")\n",
    "\n",
    "print(\"Quick sample flow setup completed!\")\n",
    "print(f\"Dataset directory: {DATASET_DIR}\")\n",
    "print(f\"CIFAR-10 path: {CIFAR10_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70994585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 sample image shapes: (5, 32, 32)\n",
      "CIFAR-10 sample label shape: (5,)\n",
      "Individual image shape: (32, 32)\n",
      "CIFAR-10 pixel value ranges: [0.0, 0.9672286510467529]\n",
      "Image data type: float32\n",
      "Label data type: <class 'int'>\n",
      "Sample datasets loaded successfully!\n",
      "CIFAR-10 sample: 100 train, 50 test\n"
     ]
    }
   ],
   "source": [
    "# Load datasets (small sample for quick demo)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    # Load datasets from cache\n",
    "    cifar10_dataset = load_cifar10_data()\n",
    "\n",
    "    # Take small samples for quick demo (100 samples each)\n",
    "    SAMPLE_SIZE = 100\n",
    "\n",
    "    # Sample from CIFAR-10\n",
    "    cifar10_sample = (\n",
    "        cifar10_dataset[\"train\"].shuffle(seed=42).select(range(SAMPLE_SIZE))\n",
    "    )\n",
    "    cifar10_test_sample = cifar10_dataset[\"test\"].shuffle(seed=42).select(range(50))\n",
    "\n",
    "    # Extract a few samples to check shapes and pixel ranges\n",
    "    sample_images = [np.array(img) for img in cifar10_sample['image'][:5]]  # First 5 images\n",
    "    sample_labels = cifar10_sample['label'][:5]  # First 5 labels\n",
    "    \n",
    "    # Convert to numpy arrays for shape analysis\n",
    "    sample_images_array = np.array(sample_images)\n",
    "    sample_labels_array = np.array(sample_labels)\n",
    "    \n",
    "    # Observe Sample Shapes\n",
    "    print(f\"CIFAR-10 sample image shapes: {sample_images_array.shape}\")  # (batch, height, width, channels)\n",
    "    print(f\"CIFAR-10 sample label shape: {sample_labels_array.shape}\")   # (batch,)\n",
    "    print(f\"Individual image shape: {sample_images[0].shape}\")           # (height, width, channels)\n",
    "    \n",
    "    # Observe Pixel Value Ranges\n",
    "    pixel_min = np.min([np.min(img) for img in sample_images])\n",
    "    pixel_max = np.max([np.max(img) for img in sample_images])\n",
    "    print(f\"CIFAR-10 pixel value ranges: [{pixel_min}, {pixel_max}]\")\n",
    "    \n",
    "    # Show data types\n",
    "    print(f\"Image data type: {sample_images[0].dtype}\")\n",
    "    print(f\"Label data type: {type(sample_labels[0])}\") # Labels are encoded class ints\n",
    "    \n",
    "    print(\"Sample datasets loaded successfully!\")\n",
    "    print(\n",
    "        f\"CIFAR-10 sample: {len(cifar10_sample)} train, {len(cifar10_test_sample)} test\"\n",
    "    )\n",
    "\n",
    "    # Aligning Variable Names with Tradition (for easier debugging)\n",
    "\n",
    "    # Split into train and test sets from a HuggingFace Dataset\n",
    "    X_train, y_train = sample_images, sample_labels\n",
    "    X_test, y_test = [np.array(img) for img in cifar10_test_sample['image']], cifar10_test_sample['label']\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    print(\"Please make sure datasets are downloaded and processed first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24b3e3",
   "metadata": {},
   "source": [
    "## Load All Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925880bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model created: DecisionTreeClassifier()\n",
      "KNN model created: KNeighborsClassifier()\n",
      "CNN model created: Backbone(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=1)\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Model classes loaded successfully!\n",
      "  Decision Tree: DecisionTreeModel\n",
      "  K-Nearest Neighbors: KNNModel\n",
      "  Convolutional Neural Network: CNNModel\n",
      "\n",
      "Total models available: 3\n"
     ]
    }
   ],
   "source": [
    "# Silence warnings to avoid printing full paths\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all model classes\n",
    "from models.decision_tree import DecisionTreeModel\n",
    "from models.knn import KNNModel\n",
    "from models.cnn import CNNModel\n",
    "\n",
    "\n",
    "def create_sample_models():\n",
    "    \"\"\"Create instances of all model classes for testing\"\"\"\n",
    "    models = {}\n",
    "\n",
    "    # Decision Tree\n",
    "    dt_model = DecisionTreeModel()\n",
    "    dt_model.create_model()\n",
    "    models[\"Decision Tree\"] = dt_model\n",
    "    print(f\"Decision Tree model created: {dt_model.estimator}\")\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    knn_model = KNNModel()\n",
    "    knn_model.create_model()\n",
    "    models[\"K-Nearest Neighbors\"] = knn_model\n",
    "    print(f\"KNN model created: {knn_model.estimator}\")\n",
    "\n",
    "    # CNN (Note: May require special handling due to PyTorch)\n",
    "    try:\n",
    "        cnn_model = CNNModel()\n",
    "        cnn_model.create_model()\n",
    "        models[\"Convolutional Neural Network\"] = cnn_model\n",
    "        print(f\"CNN model created: {cnn_model.network}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] CNN model creation failed: {e}\")\n",
    "        print(\"CNN will be skipped in quick demo\")\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# Create model instances\n",
    "sample_models = create_sample_models()\n",
    "\n",
    "print(\"Model classes loaded successfully!\")\n",
    "for name, model in sample_models.items():\n",
    "    print(f\"  {name}: {type(model).__name__}\")\n",
    "\n",
    "print(f\"\\nTotal models available: {len(sample_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d798a6",
   "metadata": {},
   "source": [
    "## Quick Hyperparameter Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367be266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_primitive(value):\n",
    "    \"\"\"Convert any complex to Python primitive types for sklearn compatibility\"\"\"\n",
    "    if hasattr(value, 'item'):  # numpy scalar\n",
    "        return value.item()\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        return value.tolist()\n",
    "    # Recursion for iterative data structures\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        return type(value)(to_primitive(v) for v in value)\n",
    "    elif isinstance(value, dict):\n",
    "        return {k: to_primitive(v) for k, v in value.items()}\n",
    "    else:\n",
    "        # Handle custom classes\n",
    "        if hasattr(value, '__dict__'):\n",
    "            return {k: to_primitive(v) for k, v in value.__dict__.items()}\n",
    "        # Already a primitive\n",
    "        return value\n",
    "\n",
    "def convert_param_by_type(value, param_type):\n",
    "    \"\"\"Convert parameter value to correct Python type based on parameter type\"\"\"\n",
    "    primitive_val = to_primitive(value)\n",
    "    if param_type == \"integer\":\n",
    "        return int(primitive_val)\n",
    "    # Note: Learning Rate is often in a logarithmic scale rather than linear\n",
    "    elif param_type == \"float\" or param_type == \"float_log\":\n",
    "        return float(primitive_val)\n",
    "    else:\n",
    "        return primitive_val\n",
    "\n",
    "def quick_hyperparameter_test(\n",
    "    models_dict, X_train, y_train, X_test, y_test, dataset_name=\"Dataset\"\n",
    "):\n",
    "    \"\"\"Perform a quick hyperparameter test with limited iterations\"\"\"\n",
    "    \n",
    "    # Import data_utils functions for CNN support\n",
    "    from framework.data_utils import create_dataloaders\n",
    "    \n",
    "    print(f\"Starting quick hyperparameter test on {dataset_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = {}\n",
    "    # Default metric results (in case of an error)\n",
    "    default_metrics = {\n",
    "        \"best_params\": None,\n",
    "        \"best_score\": -1.0,\n",
    "        \"metrics\": {\"accuracy\": -1.0},\n",
    "        \"info\": \"Skipped in quick test - errornous run for current model\"\n",
    "    }\n",
    "\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nTesting {model_name}...\")\n",
    "\n",
    "        # Check if model supports hyperparameter tuning\n",
    "        if not hasattr(model, \"get_param_space\"):\n",
    "            print(\n",
    "                f\"[WARNING] {model_name} does not support hyperparameter tuning. Using default params.\"\n",
    "            )\n",
    "            try:\n",
    "                model_copy = deepcopy(model)\n",
    "                \n",
    "                # Handle CNN models with DataLoader\n",
    "                if \"Convolutional\" in model_name or \"CNN\" in model_name:\n",
    "                    try:\n",
    "                        # Convert images to grayscale for CNN compatibility\n",
    "                        from framework.data_utils import convert_to_grayscale\n",
    "                        X_train_gray = [convert_to_grayscale(img) for img in X_train]\n",
    "                        X_test_gray = [convert_to_grayscale(img) for img in X_test]\n",
    "                        \n",
    "                        # Use project's data_utils to create DataLoaders\n",
    "                        train_loader, val_loader = create_dataloaders(\n",
    "                            X_train_gray, y_train, X_test_gray, y_test, batch_size=32\n",
    "                        )\n",
    "                        # CNN expects train_loader, val_loader as arguments\n",
    "                        model_copy.train(train_loader, val_loader)\n",
    "                        \n",
    "                        # Evaluate CNN model - models now handle ROC AUC errors internally\n",
    "                        metrics = model_copy.evaluate(val_loader, y_test)\n",
    "                    except Exception as cnn_error:\n",
    "                        print(f\"[ERROR] CNN training failed: {cnn_error}\")\n",
    "                        results[model_name] = {\"error\": f\"CNN training failed: {str(cnn_error)[:50]}...\"}\n",
    "                        continue\n",
    "                else:\n",
    "                    # For sklearn models, flatten the images to 1D  \n",
    "                    X_train_flat = np.array([img.flatten() for img in X_train])\n",
    "                    X_test_flat = np.array([img.flatten() for img in X_test])\n",
    "                    \n",
    "                    model_copy.train(X_train_flat, y_train)\n",
    "                    \n",
    "                    # Evaluate sklearn models - models now handle ROC AUC errors internally\n",
    "                    metrics = model_copy.evaluate(X_test_flat, y_test)\n",
    "\n",
    "                results[model_name] = {\n",
    "                    \"best_params\": \"default\",\n",
    "                    \"best_score\": metrics.get(\"accuracy\", default_metrics[\"metrics\"][\"accuracy\"]),\n",
    "                    \"metrics\": metrics,\n",
    "                }\n",
    "                # Observe the Accuracy from Metrics\n",
    "                acc = metrics.get('accuracy', default_metrics[\"metrics\"][\"accuracy\"])\n",
    "                print(f\"Default accuracy: {acc:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {e}\")\n",
    "                results[model_name] = {\"error\": str(e)}\n",
    "            continue\n",
    "\n",
    "        # Get parameter space and sample a few combinations\n",
    "        try:\n",
    "            param_space = model.get_param_space()\n",
    "            param_names = list(param_space.keys())\n",
    "\n",
    "            # Extract actual values from ParamSpace objects\n",
    "            def extract_param_values_for_quick_test(param_space):\n",
    "                \"\"\"Quick version of param extraction with fewer samples\"\"\"\n",
    "                param_values = []\n",
    "                param_types = []  # Track parameter types for proper conversion\n",
    "                for param_name, param_def in param_space.items():\n",
    "                    if hasattr(param_def, \"param_type\"):\n",
    "                        # This is a ParamSpace object\n",
    "                        param_type = param_def.param_type.value\n",
    "                        param_types.append(param_type)\n",
    "                        \n",
    "                        def get_param_values_numeric(param_def):\n",
    "                            \"\"\"Helper to get numeric param values for quick test\"\"\"\n",
    "                            values = []\n",
    "                            if (\n",
    "                                param_def.min_value is not None\n",
    "                                and param_def.max_value is not None\n",
    "                            ):\n",
    "                                # Note: Learning Rate is often in a logarithmic scale rather than linear\n",
    "                                # Handle logarithmic ranges for float_log type\n",
    "                                if param_type == \"float_log\":\n",
    "                                    # Use logarithmic spacing for log ranges\n",
    "                                    log_values = np.logspace(\n",
    "                                        np.log10(param_def.min_value), \n",
    "                                        np.log10(param_def.max_value), \n",
    "                                        3\n",
    "                                    )\n",
    "                                    values = [to_primitive(val) for val in log_values]\n",
    "                                else:\n",
    "                                    # Use linear spacing for regular ranges\n",
    "                                    float_values = np.linspace(\n",
    "                                        param_def.min_value, param_def.max_value, 3\n",
    "                                    )\n",
    "                                    values = [to_primitive(val) for val in float_values]\n",
    "                            if (\n",
    "                                param_def.default is not None\n",
    "                                and param_def.default not in values\n",
    "                            ):\n",
    "                                values.append(to_primitive(param_def.default))\n",
    "                            return values\n",
    "                        \n",
    "                        match param_type:\n",
    "                            case \"categorical\":\n",
    "                                values = param_def.choices\n",
    "                            case \"boolean\":\n",
    "                                values = [True, False]\n",
    "                            case \"integer\":\n",
    "                                # Sample a few integer values from the range for quick testing\n",
    "                                values = get_param_values_numeric(param_def)\n",
    "                                # Convert to integers explicitly\n",
    "                                values = [int(val) for val in values] if values else [int(to_primitive(param_def.default or 1))]\n",
    "                                \n",
    "                                # Special handling for n_neighbors in KNN to prevent sample size issues\n",
    "                                if param_name == \"n_neighbors\" and (\"K-Nearest\" in model_name or \"KNN\" in model_name):\n",
    "                                    max_neighbors = max(1, len(X_train) // 3)  # Conservative limit\n",
    "                                    values = [v for v in values if 1 <= v <= max_neighbors]\n",
    "                                    if not values:  # If all filtered out, use safe default\n",
    "                                        values = [min(3, max_neighbors)]\n",
    "                                    print(f\"[INFO] Limited n_neighbors to {values} (max: {max_neighbors})\")          \n",
    "                            case \"float\" | \"float_log\": # also cater for logarithmic learning rates\n",
    "                                # Sample a few float values from the range for quick testing\n",
    "                                # float_log uses logarithmic spacing, regular float uses linear spacing\n",
    "                                values = get_param_values_numeric(param_def)\n",
    "                                values = values if values else [to_primitive(param_def.default or 0.1)]\n",
    "                            case _:\n",
    "                                raise ValueError(f\"Unsupported param type: {param_type}\")\n",
    "                        param_values.append(values)\n",
    "                    else:\n",
    "                        # This is already a list of values\n",
    "                        param_values.append(param_def)\n",
    "                        param_types.append(None)\n",
    "\n",
    "                return param_values, param_types\n",
    "\n",
    "            param_values, param_types = extract_param_values_for_quick_test(param_space)\n",
    "\n",
    "            # Generate all combinations and sample a few\n",
    "            all_combinations = list(itertools.product(*param_values)) # Cartesian Product\n",
    "            max_test = min(3, len(all_combinations))  # Test max 3 combinations for speed\n",
    "            test_combinations = (\n",
    "                random.sample(all_combinations, max_test)\n",
    "                if len(all_combinations) > max_test\n",
    "                else all_combinations\n",
    "            )\n",
    "\n",
    "            print(f\"Testing {len(test_combinations)}/{len(all_combinations)} parameter combinations...\")\n",
    "\n",
    "            best_score = -1.0\n",
    "            best_params = None\n",
    "            best_metrics = None\n",
    "\n",
    "            # Prepare data once for efficiency\n",
    "            if \"Convolutional\" in model_name or \"CNN\" in model_name:\n",
    "                try:\n",
    "                    # Convert images to grayscale for CNN compatibility\n",
    "                    from framework.data_utils import convert_to_grayscale\n",
    "                    X_train_gray = [convert_to_grayscale(img) for img in X_train]\n",
    "                    X_test_gray = [convert_to_grayscale(img) for img in X_test]\n",
    "                        \n",
    "                    train_loader, val_loader = create_dataloaders(\n",
    "                        X_train_gray, y_train, X_test_gray, y_test, batch_size=32\n",
    "                    )\n",
    "                    X_train_prep, y_train_prep = train_loader, val_loader\n",
    "                    X_test_prep, y_test_prep = val_loader, y_test\n",
    "                except Exception as dl_error:\n",
    "                    print(f\"[ERROR] DataLoader creation failed: {dl_error}\")\n",
    "                    results[model_name] = {\"error\": f\"DataLoader failed: {str(dl_error)[:50]}...\"}\n",
    "                    continue\n",
    "            else:\n",
    "                X_train_prep = np.array([img.flatten() for img in X_train])\n",
    "                X_test_prep = np.array([img.flatten() for img in X_test])\n",
    "                y_train_prep, y_test_prep = y_train, y_test\n",
    "\n",
    "            for i, param_combo in enumerate(test_combinations):\n",
    "                current_params = dict(zip(param_names, param_combo))\n",
    "                \n",
    "                # Convert parameters with type awareness\n",
    "                for param_name, (param_value, param_type) in zip(param_names, zip(param_combo, param_types)):\n",
    "                    current_params[param_name] = convert_param_by_type(param_value, param_type)\n",
    "\n",
    "                print(f\"Combo {i + 1}: Testing params {current_params}\")\n",
    "\n",
    "                try:\n",
    "                    # Create fresh model copy\n",
    "                    model_copy = deepcopy(model)\n",
    "\n",
    "                    # Set parameters - handle sklearn, PyTorch, and custom models\n",
    "                    param_set_success = False\n",
    "                    \n",
    "                    if hasattr(model_copy, \"estimator\") and hasattr(model_copy.estimator, \"set_params\"):\n",
    "                        # sklearn models (DecisionTree, KNN, etc.)\n",
    "                        model_copy.estimator.set_params(**current_params)\n",
    "                        param_set_success = True\n",
    "                    elif \"Convolutional\" in model_name:\n",
    "                        # CNN params will be handled via create_model then train\n",
    "                        param_set_success = True\n",
    "                    elif hasattr(model_copy, \"set_params\"):\n",
    "                        # Models with direct set_params method\n",
    "                        model_copy.set_params(**current_params)\n",
    "                        param_set_success = True\n",
    "                    elif hasattr(model_copy, \"model\") and hasattr(model_copy.model, \"set_params\"):\n",
    "                        # Models with model attribute that has set_params\n",
    "                        model_copy.model.set_params(**current_params)\n",
    "                        param_set_success = True\n",
    "                    \n",
    "                    if not param_set_success:\n",
    "                            print(f\"Combo {i + 1}: ERROR - Model does not support parameter setting\")\n",
    "                            continue\n",
    "\n",
    "                    # Train model based on type\n",
    "                    if \"Convolutional\" in model_name:\n",
    "                        # First create model with parameters, then train with DataLoaders\n",
    "                        # Separate CNN-specific params from training config params\n",
    "                        cnn_params = {}\n",
    "                        training_config_params = {}\n",
    "                        \n",
    "                        for param_name, param_value in current_params.items():\n",
    "                            if param_name in ['batch_size', 'learning_rate', 'optimizer', 'weight_decay']:\n",
    "                                training_config_params[param_name] = param_value\n",
    "                            else:\n",
    "                                cnn_params[param_name] = param_value\n",
    "                        \n",
    "                        # Create model with CNN architecture params\n",
    "                        model_copy.create_model(**cnn_params)\n",
    "                        \n",
    "                        # Create training config with training params\n",
    "                        config = TrainingConfig(epochs=5, **training_config_params)\n",
    "                        \n",
    "                        # Train using the correct CNN signature: train(train_loader, val_loader, config=config)\n",
    "                        model_copy.train(X_train_prep, X_test_prep, config=config)\n",
    "                    else: \n",
    "                        model_copy.train(X_train_prep, y_train_prep)\n",
    "                    \n",
    "                    # Evaluate - models now handle ROC AUC errors internally\n",
    "                    if \"Convolutional\" in model_name:\n",
    "                        # For CNN, use the DataLoader for evaluation\n",
    "                        metrics = model_copy.evaluate(X_test_prep, y_test)\n",
    "                    else:\n",
    "                        # sklearn models use flattened arrays\n",
    "                        metrics = model_copy.evaluate(X_test_prep, y_test_prep)\n",
    "\n",
    "                    current_score = metrics.get(\"accuracy\", default_metrics[\"metrics\"][\"accuracy\"])\n",
    "                    print(f\"Combo {i + 1}: Accuracy: {current_score:.4f}\")\n",
    "\n",
    "                    if current_score > best_score:\n",
    "                        best_score = current_score\n",
    "                        best_params = current_params.copy()\n",
    "                        best_metrics = metrics.copy()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Combo {i + 1}: ERROR - {e}...\")\n",
    "                    continue\n",
    "\n",
    "            # Store results for this model\n",
    "            results[model_name] = {\n",
    "                \"best_params\": best_params,\n",
    "                \"best_score\": best_score,\n",
    "                \"metrics\": best_metrics or default_metrics[\"metrics\"],\n",
    "            }\n",
    "\n",
    "            if best_params:\n",
    "                print(f\"Best params: {best_params}\")\n",
    "                print(f\"Best score: {best_score:.4f}\")\n",
    "            else:\n",
    "                print(\"No successful parameter combinations found!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to test hyperparameters for {model_name}: {str(e)[:80]}...\")\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Quick Hyperparameter Test Summary:\")\n",
    "    for model_name, result in results.items():\n",
    "        if \"error\" in result:\n",
    "            print(f\"{model_name}: ERROR - {result['error']}\")\n",
    "        else:\n",
    "            score = result.get(\"best_score\", -1.0)\n",
    "            print(f\"{model_name}: Best Score = {score:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b263d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing all models on CIFAR-10 sample...\n",
      "Starting quick hyperparameter test on CIFAR-10\n",
      "============================================================\n",
      "\n",
      "Testing Decision Tree...\n",
      "Testing 3/128 parameter combinations...\n",
      "Combo 1: Testing params {'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 5, 'criterion': 'entropy'}\n",
      "Combo 1: Accuracy: 0.1600\n",
      "Combo 2: Testing params {'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 5, 'criterion': 'entropy'}\n",
      "Combo 2: Accuracy: 0.1600\n",
      "Combo 3: Testing params {'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5, 'criterion': 'gini'}\n",
      "Combo 3: Accuracy: 0.1600\n",
      "Best params: {'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 5, 'criterion': 'entropy'}\n",
      "Best score: 0.1600\n",
      "\n",
      "Testing K-Nearest Neighbors...\n",
      "[INFO] Limited n_neighbors to [1] (max: 1)\n",
      "Testing 3/4 parameter combinations...\n",
      "Combo 1: Testing params {'n_neighbors': 1, 'weights': 'uniform', 'metric': 'manhattan'}\n",
      "Combo 1: Accuracy: 0.0800\n",
      "Combo 2: Testing params {'n_neighbors': 1, 'weights': 'distance', 'metric': 'minkowski'}\n",
      "Combo 2: Accuracy: 0.0600\n",
      "Combo 3: Testing params {'n_neighbors': 1, 'weights': 'uniform', 'metric': 'minkowski'}\n",
      "Combo 3: Accuracy: 0.0600\n",
      "Best params: {'n_neighbors': 1, 'weights': 'uniform', 'metric': 'manhattan'}\n",
      "Best score: 0.0800\n",
      "\n",
      "Testing Convolutional Neural Network...\n",
      "Testing 3/1152 parameter combinations...\n",
      "Combo 1: Testing params {'kernel_size': 4, 'stride': 3, 'learning_rate': 0.00031622776601683794, 'batch_size': 16, 'weight_decay': 0.005, 'optimizer': 'SGD'}\n",
      "Combo 1: Accuracy: 0.0800\n",
      "Combo 2: Testing params {'n_neighbors': 1, 'weights': 'distance', 'metric': 'minkowski'}\n",
      "Combo 2: Accuracy: 0.0600\n",
      "Combo 3: Testing params {'n_neighbors': 1, 'weights': 'uniform', 'metric': 'minkowski'}\n",
      "Combo 3: Accuracy: 0.0600\n",
      "Best params: {'n_neighbors': 1, 'weights': 'uniform', 'metric': 'manhattan'}\n",
      "Best score: 0.0800\n",
      "\n",
      "Testing Convolutional Neural Network...\n",
      "Testing 3/1152 parameter combinations...\n",
      "Combo 1: Testing params {'kernel_size': 4, 'stride': 3, 'learning_rate': 0.00031622776601683794, 'batch_size': 16, 'weight_decay': 0.005, 'optimizer': 'SGD'}\n",
      "Total parameters: 42,202\n",
      "Trainable parameters: 42,202\n",
      "\n",
      "Epoch 1/5\n",
      "Total parameters: 42,202\n",
      "Trainable parameters: 42,202\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3706, Train Acc: 0.0000 (0.00%)\n",
      "Val Loss: 2.2940, Val Acc: 0.1200 (12.00%)\n",
      "Saved best model (val_acc=0.1200) to .cache\\models\\cnn_cifar.pth\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3705, Train Acc: 0.0000 (0.00%)\n",
      "Val Loss: 2.2954, Val Acc: 0.1200 (12.00%)\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3620, Train Acc: 0.0000 (0.00%)\n",
      "Val Loss: 2.2965, Val Acc: 0.1200 (12.00%)\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3544, Train Acc: 0.0000 (0.00%)\n",
      "Val Loss: 2.2977, Val Acc: 0.1200 (12.00%)\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3513, Train Acc: 0.0000 (0.00%)\n",
      "Val Loss: 2.2990, Val Acc: 0.1000 (10.00%)\n",
      "\n",
      "Training complete!\n",
      "Best val acc: 0.1200 (12.00%)\n",
      "Combo 1: Accuracy: 0.1000\n",
      "Combo 2: Testing params {'kernel_size': 5, 'stride': 1, 'learning_rate': 0.0003, 'batch_size': 64, 'weight_decay': 0.005, 'optimizer': 'AdamW'}\n",
      "Total parameters: 65,386\n",
      "Trainable parameters: 65,386\n",
      "\n",
      "Epoch 1/5\n",
      "Combo 1: Accuracy: 0.1000\n",
      "Combo 2: Testing params {'kernel_size': 5, 'stride': 1, 'learning_rate': 0.0003, 'batch_size': 64, 'weight_decay': 0.005, 'optimizer': 'AdamW'}\n",
      "Total parameters: 65,386\n",
      "Trainable parameters: 65,386\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2826, Train Acc: 0.2000 (20.00%)\n",
      "Val Loss: 2.2968, Val Acc: 0.1200 (12.00%)\n",
      "Saved best model (val_acc=0.1200) to .cache\\models\\cnn_cifar.pth\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2780, Train Acc: 0.2000 (20.00%)\n",
      "Val Loss: 2.2990, Val Acc: 0.1000 (10.00%)\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2036, Train Acc: 0.2000 (20.00%)\n",
      "Val Loss: 2.3003, Val Acc: 0.0800 (8.00%)\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1435, Train Acc: 0.2000 (20.00%)\n",
      "Val Loss: 2.3003, Val Acc: 0.0800 (8.00%)\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1185, Train Acc: 0.2000 (20.00%)\n",
      "Val Loss: 2.2997, Val Acc: 0.0800 (8.00%)\n",
      "\n",
      "Training complete!\n",
      "Best val acc: 0.1200 (12.00%)\n",
      "Combo 2: Accuracy: 0.0800\n",
      "Combo 3: Testing params {'kernel_size': 3, 'stride': 3, 'learning_rate': 0.00031622776601683794, 'batch_size': 32, 'weight_decay': 0.0, 'optimizer': 'AdamW'}\n",
      "Total parameters: 24,170\n",
      "Trainable parameters: 24,170\n",
      "\n",
      "Epoch 1/5\n",
      "Combo 2: Accuracy: 0.0800\n",
      "Combo 3: Testing params {'kernel_size': 3, 'stride': 3, 'learning_rate': 0.00031622776601683794, 'batch_size': 32, 'weight_decay': 0.0, 'optimizer': 'AdamW'}\n",
      "Total parameters: 24,170\n",
      "Trainable parameters: 24,170\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4491, Train Acc: 0.0000 (0.00%)\n",
      "Val Loss: 2.3043, Val Acc: 0.1000 (10.00%)\n",
      "Saved best model (val_acc=0.1000) to .cache\\models\\cnn_cifar.pth\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4415, Train Acc: 0.0000 (0.00%)\n",
      "Val Loss: 2.3048, Val Acc: 0.1000 (10.00%)\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3202, Train Acc: 0.0000 (0.00%)\n",
      "Val Loss: 2.3045, Val Acc: 0.1200 (12.00%)\n",
      "Saved best model (val_acc=0.1200) to .cache\\models\\cnn_cifar.pth\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2365, Train Acc: 0.0000 (0.00%)\n",
      "Val Loss: 2.3038, Val Acc: 0.0600 (6.00%)\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2023, Train Acc: 0.0000 (0.00%)\n",
      "Val Loss: 2.3029, Val Acc: 0.1000 (10.00%)\n",
      "\n",
      "Training complete!\n",
      "Best val acc: 0.1200 (12.00%)\n",
      "Combo 3: Accuracy: 0.1000\n",
      "Best params: {'kernel_size': 4, 'stride': 3, 'learning_rate': 0.00031622776601683794, 'batch_size': 16, 'weight_decay': 0.005, 'optimizer': 'SGD'}\n",
      "Best score: 0.1000\n",
      "\n",
      "============================================================\n",
      "Quick Hyperparameter Test Summary:\n",
      "Decision Tree: Best Score = 0.1600\n",
      "K-Nearest Neighbors: Best Score = 0.0800\n",
      "Convolutional Neural Network: Best Score = 0.1000\n",
      "\n",
      "================================================================================\n",
      "QUICK TEST RESULTS SUMMARY - CIFAR-10\n",
      "================================================================================\n",
      "[RESULT] Decision Tree: Accuracy = 0.1600\n",
      "[RESULT] K-Nearest Neighbors: Accuracy = 0.0800\n",
      "[RESULT] Convolutional Neural Network: Accuracy = 0.1000\n",
      "Combo 3: Accuracy: 0.1000\n",
      "Best params: {'kernel_size': 4, 'stride': 3, 'learning_rate': 0.00031622776601683794, 'batch_size': 16, 'weight_decay': 0.005, 'optimizer': 'SGD'}\n",
      "Best score: 0.1000\n",
      "\n",
      "============================================================\n",
      "Quick Hyperparameter Test Summary:\n",
      "Decision Tree: Best Score = 0.1600\n",
      "K-Nearest Neighbors: Best Score = 0.0800\n",
      "Convolutional Neural Network: Best Score = 0.1000\n",
      "\n",
      "================================================================================\n",
      "QUICK TEST RESULTS SUMMARY - CIFAR-10\n",
      "================================================================================\n",
      "[RESULT] Decision Tree: Accuracy = 0.1600\n",
      "[RESULT] K-Nearest Neighbors: Accuracy = 0.0800\n",
      "[RESULT] Convolutional Neural Network: Accuracy = 0.1000\n"
     ]
    }
   ],
   "source": [
    "# Test on CIFAR-10 only\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing all models on CIFAR-10 sample...\")\n",
    "cifar_results = quick_hyperparameter_test(\n",
    "    sample_models, X_train, y_train, X_test, y_test, \"CIFAR-10\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUICK TEST RESULTS SUMMARY - CIFAR-10\")\n",
    "print(\"=\" * 80)\n",
    "for model_name, result in cifar_results.items():\n",
    "    if \"error\" in result:\n",
    "        print(f\"[ERROR] {model_name} failed - {result['error']}\")\n",
    "    else:\n",
    "        print(f\"[RESULT] {model_name}: Accuracy = {result['best_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58233334",
   "metadata": {},
   "source": [
    "Noticing the exceptionally low scores, it could be the reason of the **quick** test. It only uses 100 training samples to quickly verify whether the pipeline works normally. So now, we go ahead with a more detailed flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73766f0",
   "metadata": {},
   "source": [
    "## Model Interface Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd785e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying model interfaces...\n",
      "==================================================\n",
      "\n",
      "Decision Tree:\n",
      "create_model()\n",
      "get_param_space()\n",
      "train()\n",
      "predict()\n",
      "evaluate()\n",
      "Parameter space: 4 parameters\n",
      "- max_depth: integer\n",
      "- min_samples_split: integer\n",
      "- min_samples_leaf: integer\n",
      "- criterion: categorical\n",
      "Model instance: DecisionTreeClassifier\n",
      "[RESULT] INTERFACE COMPLETE\n",
      "\n",
      "K-Nearest Neighbors:\n",
      "create_model()\n",
      "get_param_space()\n",
      "train()\n",
      "predict()\n",
      "evaluate()\n",
      "Parameter space: 3 parameters\n",
      "- n_neighbors: integer\n",
      "- weights: categorical\n",
      "- metric: categorical\n",
      "Model instance: KNeighborsClassifier\n",
      "[RESULT] INTERFACE COMPLETE\n",
      "\n",
      "Convolutional Neural Network:\n",
      "create_model()\n",
      "get_param_space()\n",
      "train()\n",
      "predict()\n",
      "evaluate()\n",
      "Parameter space: 6 parameters\n",
      "- kernel_size: integer\n",
      "- stride: integer\n",
      "- learning_rate: float_log\n",
      "- batch_size: categorical\n",
      "- weight_decay: float\n",
      "- optimizer: categorical\n",
      "Model instance: Backbone\n",
      "[RESULT] INTERFACE COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Verify that all models implement the required interface correctly\n",
    "def verify_model_interface(model_dict):\n",
    "    \"\"\"Verify that all models implement the BaseModel interface correctly\"\"\"\n",
    "    print(\"Verifying model interfaces...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for name, model in model_dict.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "\n",
    "        # Check required methods\n",
    "        required_methods = [\n",
    "            \"create_model\",\n",
    "            \"get_param_space\",\n",
    "            \"train\",\n",
    "            \"predict\",\n",
    "            \"evaluate\",\n",
    "        ]\n",
    "        missing_methods = []\n",
    "\n",
    "        for method in required_methods:\n",
    "            if hasattr(model, method):\n",
    "                print(f\"{method}()\")\n",
    "            else:\n",
    "                print(f\"[WARNING] {method}() - MISSING\")\n",
    "                missing_methods.append(method)\n",
    "\n",
    "        # Check parameter space\n",
    "        try:\n",
    "            if hasattr(model, \"get_param_space\"):\n",
    "                param_space = model.get_param_space()\n",
    "                print(f\"Parameter space: {len(param_space)} parameters\")\n",
    "                for param_name, param_def in param_space.items():\n",
    "                    print(f\"- {param_name}: {param_def.param_type.value}\")\n",
    "            else:\n",
    "                print(\"[WARNING] No parameter space available\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Parameter space error: {e}\")\n",
    "\n",
    "        # Check if model is created\n",
    "        if hasattr(model, \"model\") and model.model is not None:\n",
    "            print(f\"Model instance: {type(model.model).__name__}\")\n",
    "        elif hasattr(model, \"estimator\") and model.estimator is not None:\n",
    "            print(f\"Model instance: {type(model.estimator).__name__}\")\n",
    "        elif hasattr(model, \"network\") and model.network is not None:\n",
    "            print(f\"Model instance: {type(model.network).__name__}\")\n",
    "        else:\n",
    "            print(\"No model instance found\")\n",
    "\n",
    "        if missing_methods:\n",
    "            print(f\"[WARNING] INTERFACE INCOMPLETE: Missing {missing_methods}\")\n",
    "        else:\n",
    "            print(\"[RESULT] INTERFACE COMPLETE\")\n",
    "\n",
    "\n",
    "# Run interface verification\n",
    "verify_model_interface(sample_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033742c3",
   "metadata": {},
   "source": [
    "## Quick Demo Summary\n",
    "\n",
    "This quick sample flow demonstrates:\n",
    "\n",
    "1. **All Model Classes Loaded**: Successfully imported and instantiated all 5 model classes from the `models/` directory\n",
    "2. **Interface Compliance**: Verified that all models implement the required `BaseModel` interface\n",
    "3. **Hyperparameter Testing**: Tested hyperparameter tuning functionality with small sample data\n",
    "4. **Training & Evaluation**: Confirmed that all models can train and evaluate on both MNIST and CIFAR-10 data\n",
    "\n",
    "### Models Tested:\n",
    "- **Decision Tree Model** (`DecisionTreeModel`)\n",
    "- **K-Nearest Neighbors Model** (`KNNModel`)\n",
    "- **Convolutional Neural Network Model** (`CNNModel`)\n",
    "\n",
    "This quick flow uses small sample sizes (100 training, 50 test samples) and limited hyperparameter combinations (max 3 per model) to ensure fast execution while still validating that the complete machine learning pipeline works correctly for all implemented model classes.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: For full experiments, use the comprehensive workflow sections below with complete datasets and extensive hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0187a2c7",
   "metadata": {},
   "source": [
    "## Model Interface Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf77f",
   "metadata": {},
   "source": [
    "# Establishing an Ordinary Model Training Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8ecb8",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7edc02c",
   "metadata": {},
   "source": [
    "Let's try on 2 different image datasets - CIFAR-10 and MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b7c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATASET_DIR = os.path.join(os.path.curdir, \"..\", \".cache\", \"processed_datasets\")\n",
    "CIFAR10_PATH = os.path.join(DATASET_DIR, \"cifar10\")\n",
    "MNIST_PATH = os.path.join(DATASET_DIR, \"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e288911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets from cache:\n",
      "CIFAR-10 Train: 50,000 examples\n",
      "CIFAR-10 Test: 10,000 examples\n",
      "MNIST Train: 60,000 examples\n",
      "MNIST Test: 10,000 examples\n",
      "\n",
      "CIFAR-10 classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "MNIST classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Load from HuggingFace datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load all datasets from cache\n",
    "cifar10_dataset = load_from_disk(CIFAR10_PATH)\n",
    "mnist_dataset = load_from_disk(MNIST_PATH)\n",
    "\n",
    "# Access train and test splits for all datasets\n",
    "CIFAR10_TRAIN = cifar10_dataset[\"train\"]\n",
    "CIFAR10_TEST = cifar10_dataset[\"test\"]\n",
    "MNIST_TRAIN = mnist_dataset[\"train\"]\n",
    "MNIST_TEST = mnist_dataset[\"test\"]\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Loaded datasets from cache:\")\n",
    "print(f\"CIFAR-10 Train: {len(CIFAR10_TRAIN):,} examples\")\n",
    "print(f\"CIFAR-10 Test: {len(CIFAR10_TEST):,} examples\")\n",
    "print(f\"MNIST Train: {len(MNIST_TRAIN):,} examples\")\n",
    "print(f\"MNIST Test: {len(MNIST_TEST):,} examples\")\n",
    "print(f\"\\nCIFAR-10 classes: {CIFAR10_TRAIN.features['label'].names}\")\n",
    "print(f\"MNIST classes: {list(range(10))}\")  # MNIST has digits 0-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c09ae5",
   "metadata": {},
   "source": [
    "## Prepare a Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b93f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After splitting into Train and Validation sets:\n",
      "CIFAR-10 Train: 49,500 examples\n",
      "CIFAR-10 Validation: 500 examples\n",
      "MNIST Train: 59,500 examples\n",
      "MNIST Validation: 500 examples\n"
     ]
    }
   ],
   "source": [
    "# Split using HuggingFace datasets train_test_split method\n",
    "cifar10_split = CIFAR10_TRAIN.train_test_split(test_size=500, seed=42)\n",
    "CIFAR10_TRAIN = cifar10_split[\"train\"]\n",
    "CIFAR10_VAL = cifar10_split[\"test\"]\n",
    "\n",
    "mnist_split = MNIST_TRAIN.train_test_split(test_size=500, seed=42)\n",
    "MNIST_TRAIN = mnist_split[\"train\"]\n",
    "MNIST_VAL = mnist_split[\"test\"]\n",
    "\n",
    "# Inspect the sizes\n",
    "print(\"After splitting into Train and Validation sets:\")\n",
    "print(f\"CIFAR-10 Train: {len(CIFAR10_TRAIN):,} examples\")\n",
    "print(f\"CIFAR-10 Validation: {len(CIFAR10_VAL):,} examples\")\n",
    "print(f\"MNIST Train: {len(MNIST_TRAIN):,} examples\")\n",
    "print(f\"MNIST Validation: {len(MNIST_VAL):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed17280",
   "metadata": {},
   "source": [
    "## Get the Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e7ecff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model: DecisionTreeModel\n",
      "K-Nearest Neighbors Model: KNNModel\n",
      "Convolutional Neural Network Model: CNNModel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Decision Tree Model': <models.decision_tree.DecisionTreeModel at 0x19c01b04510>,\n",
       " 'K-Nearest Neighbors Model': <models.knn.KNNModel at 0x19b8f2c3230>,\n",
       " 'Convolutional Neural Network Model': <models.cnn.CNNModel at 0x19b8f2c3490>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.curdir, \"..\"))\n",
    "\n",
    "\n",
    "# Instantiate model classes (not the sklearn models directly)\n",
    "dt_model = DecisionTreeModel()\n",
    "dt_model.create_model()\n",
    "knn_model = KNNModel()\n",
    "knn_model.create_model()\n",
    "cnn_model = CNNModel()\n",
    "cnn_model.create_model()\n",
    "\n",
    "# Display those model instances\n",
    "models = {\n",
    "    \"Decision Tree Model\": dt_model,\n",
    "    \"K-Nearest Neighbors Model\": knn_model,\n",
    "    \"Convolutional Neural Network Model\": cnn_model,\n",
    "}\n",
    "\n",
    "# Show the model classes\n",
    "for name, model in models.items():\n",
    "    print(f\"{name}: {type(model).__name__}\")\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d878aa",
   "metadata": {},
   "source": [
    "## Tune the Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060068c",
   "metadata": {},
   "source": [
    "We tune it with our own multi-objective fitness functions across different metaheuristics. For simlicity, let's just demonstrate how we weigh the importances of every metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac6da5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Validation (Grayscale):\n",
      "X_VAL shape: (500, 1024)\n",
      "y_VAL shape: (500,)\n",
      "Data type: X=float32, y=int64\n",
      "\n",
      "MNIST Validation (Grayscale):\n",
      "X_VAL shape: (500, 1024)\n",
      "y_VAL shape: (500,)\n",
      "Data type: X=float32, y=int64\n",
      "\n",
      "CIFAR-10 label examples: [1 2 6 7 9]\n",
      "MNIST label examples: [8 7 1 6 0]\n",
      "Pixel value range CIFAR-10: [0.0, 1.0]\n",
      "Pixel value range MNIST: [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Define the extract_features_and_labels function to extract images and labels from HuggingFace datasets\n",
    "def extract_features_and_labels(dataset, flatten_images=True, max_samples=None, convert_to_grayscale=False):\n",
    "    \"\"\"Extract features and labels from HuggingFace dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with 'image' and 'label' columns\n",
    "        flatten_images: Whether to flatten images to 1D arrays (for sklearn models)\n",
    "        max_samples: Maximum number of samples to extract (None for all)\n",
    "        convert_to_grayscale: Whether to convert images to grayscale (for CNN compatibility)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) where X is image features and y is labels\n",
    "    \"\"\"\n",
    "    # Import grayscale conversion utility\n",
    "    from framework.data_utils import convert_to_grayscale\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    if max_samples is not None and len(dataset) > max_samples:\n",
    "        dataset = dataset.select(range(max_samples))\n",
    "    \n",
    "    # Extract images\n",
    "    X = []\n",
    "    for img in dataset['image']:\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Convert to grayscale if requested\n",
    "        if convert_to_grayscale:\n",
    "            img_array = convert_to_grayscale(img_array)\n",
    "        \n",
    "        if flatten_images:\n",
    "            img_array = img_array.flatten()\n",
    "        X.append(img_array)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(dataset['label'])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Extract validation features and labels for CIFAR-10 (convert to grayscale for CNN compatibility)\n",
    "X_CIFAR10_VAL, y_CIFAR10_VAL = extract_features_and_labels(\n",
    "    CIFAR10_VAL, flatten_images=True, convert_to_grayscale=True\n",
    ")\n",
    "print(\"CIFAR-10 Validation (Grayscale):\")\n",
    "print(f\"X_VAL shape: {X_CIFAR10_VAL.shape}\")\n",
    "print(f\"y_VAL shape: {y_CIFAR10_VAL.shape}\")\n",
    "print(f\"Data type: X={X_CIFAR10_VAL.dtype}, y={y_CIFAR10_VAL.dtype}\")\n",
    "\n",
    "# Extract validation features and labels for MNIST (already grayscale)\n",
    "X_MNIST_VAL, y_MNIST_VAL = extract_features_and_labels(MNIST_VAL, flatten_images=True, convert_to_grayscale=True)\n",
    "print(\"\\nMNIST Validation (Grayscale):\")\n",
    "print(f\"X_VAL shape: {X_MNIST_VAL.shape}\")\n",
    "print(f\"y_VAL shape: {y_MNIST_VAL.shape}\")\n",
    "print(f\"Data type: X={X_MNIST_VAL.dtype}, y={y_MNIST_VAL.dtype}\")\n",
    "\n",
    "# Show label examples\n",
    "print(f\"\\nCIFAR-10 label examples: {y_CIFAR10_VAL[:5]}\")\n",
    "print(f\"MNIST label examples: {y_MNIST_VAL[:5]}\")\n",
    "print(f\"Pixel value range CIFAR-10: [{X_CIFAR10_VAL.min()}, {X_CIFAR10_VAL.max()}]\")\n",
    "print(f\"Pixel value range MNIST: [{X_MNIST_VAL.min()}, {X_MNIST_VAL.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc45f5",
   "metadata": {},
   "source": [
    "The cell below shows 3 alternative methods of extracting the images array and the labels array, just as a reference to check whether the above extraction behaves correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5366929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Alternative Method 1: Direct Column Access ===\n",
      "y_CIFAR10_VAL shape: (500,)\n",
      "\n",
      "=== Alternative Method 2: Using to_pandas() ===\n",
      "DataFrame shape: (500, 2)\n",
      "DataFrame columns: ['image', 'label']\n",
      "\n",
      "=== Alternative Method 3: Batch Processing ===\n",
      "Batch extraction example - X shape: (100, 1024), y shape: (100,)\n",
      "y_CIFAR10_VAL shape: (500,)\n",
      "\n",
      "=== Alternative Method 2: Using to_pandas() ===\n",
      "DataFrame shape: (500, 2)\n",
      "DataFrame columns: ['image', 'label']\n",
      "\n",
      "=== Alternative Method 3: Batch Processing ===\n",
      "Batch extraction example - X shape: (100, 1024), y shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Alternative methods for extracting X and y\n",
    "\n",
    "# Method 1: Direct column access (simpler but less flexible)\n",
    "print(\"=== Alternative Method 1: Direct Column Access ===\")\n",
    "y_cifar_simple = np.array(CIFAR10_VAL[\"label\"])\n",
    "print(f\"y_CIFAR10_VAL shape: {y_cifar_simple.shape}\")\n",
    "\n",
    "# Method 2: Using dataset.to_pandas()\n",
    "print(\"\\n=== Alternative Method 2: Using to_pandas() ===\")\n",
    "cifar_df = CIFAR10_VAL.to_pandas()\n",
    "print(f\"DataFrame shape: {cifar_df.shape}\")\n",
    "print(f\"DataFrame columns: {list(cifar_df.columns)}\")\n",
    "\n",
    "# Method 3: Batch processing for large datasets (memory efficient)\n",
    "print(\"\\n=== Alternative Method 3: Batch Processing ===\")\n",
    "\n",
    "\n",
    "def extract_in_batches(dataset, batch_size=1000, flatten_images=True):\n",
    "    \"\"\"Extract features and labels in batches to save memory\"\"\"\n",
    "    total_samples = len(dataset)\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch = dataset[i : i + batch_size]\n",
    "\n",
    "        # Process batch\n",
    "        batch_images = []\n",
    "        for img in batch[\"image\"]:\n",
    "            img_array = np.array(img)\n",
    "            if flatten_images:\n",
    "                img_array = img_array.flatten()\n",
    "            batch_images.append(img_array)\n",
    "\n",
    "        X_batches.append(np.array(batch_images))\n",
    "        y_batches.append(np.array(batch[\"label\"]))\n",
    "\n",
    "    # Concatenate all batches\n",
    "    X = np.vstack(X_batches)\n",
    "    y = np.concatenate(y_batches)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Example with small batch for demonstration\n",
    "X_batch, y_batch = extract_in_batches(CIFAR10_VAL.select(range(100)), batch_size=50)\n",
    "print(f\"Batch extraction example - X shape: {X_batch.shape}, y shape: {y_batch.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperparameter-tuning-search-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
