{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90a756a",
   "metadata": {},
   "source": [
    "# Quick Sample Flow - All Models Demo\n",
    "\n",
    "This section demonstrates a quick sample run of all implemented model classes with minimal data and iterations to verify everything works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8d72b",
   "metadata": {},
   "source": [
    "## Quick Setup - Load Minimal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "\n",
    "# Add parent directory to path to import our models\n",
    "sys.path.append(os.path.join(os.path.curdir, \"..\"))\n",
    "\n",
    "# Dataset directories\n",
    "DATASET_DIR = os.path.join(os.path.curdir, \"..\", \".cache\", \"processed_datasets\")\n",
    "CIFAR10_PATH = os.path.join(DATASET_DIR, \"cifar10\")\n",
    "MNIST_PATH = os.path.join(DATASET_DIR, \"mnist\")\n",
    "\n",
    "print(\"Quick sample flow setup completed!\")\n",
    "print(f\"Dataset directory: {DATASET_DIR}\")\n",
    "print(f\"CIFAR-10 path: {CIFAR10_PATH}\")\n",
    "print(f\"MNIST path: {MNIST_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70994585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets (small sample for quick demo)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "try:\n",
    "    # Load datasets from cache\n",
    "    cifar10_dataset = load_from_disk(CIFAR10_PATH)\n",
    "    mnist_dataset = load_from_disk(MNIST_PATH)\n",
    "\n",
    "    # Take small samples for quick demo (100 samples each)\n",
    "    SAMPLE_SIZE = 100\n",
    "\n",
    "    # Sample from CIFAR-10\n",
    "    cifar10_sample = (\n",
    "        cifar10_dataset[\"train\"].shuffle(seed=42).select(range(SAMPLE_SIZE))\n",
    "    )\n",
    "    cifar10_test_sample = cifar10_dataset[\"test\"].shuffle(seed=42).select(range(50))\n",
    "\n",
    "    # Sample from MNIST\n",
    "    mnist_sample = mnist_dataset[\"train\"].shuffle(seed=42).select(range(SAMPLE_SIZE))\n",
    "    mnist_test_sample = mnist_dataset[\"test\"].shuffle(seed=42).select(range(50))\n",
    "\n",
    "    print(\"Sample datasets loaded successfully!\")\n",
    "    print(\n",
    "        f\"CIFAR-10 sample: {len(cifar10_sample)} train, {len(cifar10_test_sample)} test\"\n",
    "    )\n",
    "    print(f\"MNIST sample: {len(mnist_sample)} train, {len(mnist_test_sample)} test\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    print(\"Please make sure datasets are downloaded and processed first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff49c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction utility function\n",
    "def extract_features_and_labels(dataset, flatten_images=True, max_samples=None):\n",
    "    \"\"\"Extract features (X) and labels (y) from a HuggingFace Dataset\"\"\"\n",
    "    images = dataset[\"image\"]\n",
    "    labels = dataset[\"label\"]\n",
    "\n",
    "    # Limit samples if specified\n",
    "    if max_samples is not None:\n",
    "        images = images[:max_samples]\n",
    "        labels = labels[:max_samples]\n",
    "\n",
    "    # Convert images to numpy arrays\n",
    "    X = []\n",
    "    for img in images:\n",
    "        if isinstance(img, Image.Image):\n",
    "            img_array = np.array(img)\n",
    "        else:\n",
    "            img_array = img\n",
    "\n",
    "        if flatten_images:\n",
    "            img_array = img_array.flatten()\n",
    "\n",
    "        X.append(img_array)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Extract sample data\n",
    "X_cifar, y_cifar = extract_features_and_labels(cifar10_sample, flatten_images=True)\n",
    "X_cifar_test, y_cifar_test = extract_features_and_labels(\n",
    "    cifar10_test_sample, flatten_images=True\n",
    ")\n",
    "\n",
    "X_mnist, y_mnist = extract_features_and_labels(mnist_sample, flatten_images=True)\n",
    "X_mnist_test, y_mnist_test = extract_features_and_labels(\n",
    "    mnist_test_sample, flatten_images=True\n",
    ")\n",
    "\n",
    "print(f\"CIFAR-10 sample shapes: X={X_cifar.shape}, y={y_cifar.shape}\")\n",
    "print(f\"MNIST sample shapes: X={X_mnist.shape}, y={y_mnist.shape}\")\n",
    "print(\n",
    "    f\"Pixel value ranges - CIFAR-10: [{X_cifar.min()}, {X_cifar.max()}], MNIST: [{X_mnist.min()}, {X_mnist.max()}]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24b3e3",
   "metadata": {},
   "source": [
    "## Load All Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925880bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence warnings to avoid printing full paths\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all model classes\n",
    "from models.decision_tree import DecisionTreeModel\n",
    "from models.knn import KNNModel\n",
    "from models.logistic_regression import LogisticRegressionModel\n",
    "from models.mlp import MLPModel\n",
    "from models.cnn import CNNModel\n",
    "\n",
    "\n",
    "def create_sample_models():\n",
    "    \"\"\"Create instances of all model classes for testing\"\"\"\n",
    "    models = {}\n",
    "\n",
    "    # Decision Tree\n",
    "    dt_model = DecisionTreeModel()\n",
    "    dt_model.create_model()\n",
    "    models[\"Decision Tree\"] = dt_model\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    knn_model = KNNModel()\n",
    "    knn_model.create_model()\n",
    "    models[\"K-Nearest Neighbors\"] = knn_model\n",
    "\n",
    "    # Logistic Regression (Logistic Regression for classification)\n",
    "    lr_model = LogisticRegressionModel()\n",
    "    lr_model.create_model()\n",
    "    models[\"Logistic Regression\"] = lr_model\n",
    "\n",
    "    # Multi-Layer Perceptron\n",
    "    mlp_model = MLPModel()\n",
    "    mlp_model.create_model()\n",
    "    models[\"Multi-Layer Perceptron\"] = mlp_model\n",
    "\n",
    "    # CNN (Note: May require special handling due to PyTorch)\n",
    "    try:\n",
    "        cnn_model = CNNModel()\n",
    "        cnn_model.create_model()\n",
    "        models[\"Convolutional Neural Network\"] = cnn_model\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] CNN model creation failed: {e}\")\n",
    "        print(\"CNN will be skipped in quick demo\")\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# Create model instances\n",
    "sample_models = create_sample_models()\n",
    "\n",
    "print(\"Model classes loaded successfully!\")\n",
    "for name, model in sample_models.items():\n",
    "    print(f\"  {name}: {type(model).__name__}\")\n",
    "\n",
    "print(f\"\\nTotal models available: {len(sample_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d798a6",
   "metadata": {},
   "source": [
    "## Quick Hyperparameter Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367be266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "\n",
    "def get_class_names_from_dataset(dataset_name):\n",
    "    \"\"\"Extract class names from HuggingFace dataset features\"\"\"\n",
    "    try:\n",
    "        if dataset_name == \"MNIST\":\n",
    "            # Try to get from MNIST dataset features\n",
    "            if (\n",
    "                hasattr(mnist_dataset[\"train\"].features[\"label\"], \"names\")\n",
    "                and mnist_dataset[\"train\"].features[\"label\"].names\n",
    "            ):\n",
    "                return mnist_dataset[\"train\"].features[\"label\"].names\n",
    "            else:\n",
    "                # MNIST typically uses digits 0-9\n",
    "                return [str(i) for i in range(10)]\n",
    "        elif dataset_name == \"CIFAR-10\":\n",
    "            # Try to get from CIFAR-10 dataset features\n",
    "            if (\n",
    "                hasattr(cifar10_dataset[\"train\"].features[\"label\"], \"names\")\n",
    "                and cifar10_dataset[\"train\"].features[\"label\"].names\n",
    "            ):\n",
    "                return cifar10_dataset[\"train\"].features[\"label\"].names\n",
    "            else:\n",
    "                # CIFAR-10 fallback class names\n",
    "                return [\n",
    "                    \"airplane\",\n",
    "                    \"automobile\",\n",
    "                    \"bird\",\n",
    "                    \"cat\",\n",
    "                    \"deer\",\n",
    "                    \"dog\",\n",
    "                    \"frog\",\n",
    "                    \"horse\",\n",
    "                    \"ship\",\n",
    "                    \"truck\",\n",
    "                ]\n",
    "        else:\n",
    "            # For other datasets, create generic class names based on unique labels\n",
    "            return [f\"Class {i}\" for i in range(10)]  # Default to 10 classes\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not extract class names for {dataset_name}: {e}\")\n",
    "        # Fallback to generic class names\n",
    "        if dataset_name == \"MNIST\":\n",
    "            return [str(i) for i in range(10)]\n",
    "        elif dataset_name == \"CIFAR-10\":\n",
    "            return [\n",
    "                \"airplane\",\n",
    "                \"automobile\",\n",
    "                \"bird\",\n",
    "                \"cat\",\n",
    "                \"deer\",\n",
    "                \"dog\",\n",
    "                \"frog\",\n",
    "                \"horse\",\n",
    "                \"ship\",\n",
    "                \"truck\",\n",
    "            ]\n",
    "        else:\n",
    "            return [f\"Class {i}\" for i in range(10)]\n",
    "\n",
    "\n",
    "def quick_hyperparameter_test(\n",
    "    models_dict, X_train, y_train, X_test, y_test, dataset_name=\"Dataset\"\n",
    "):\n",
    "    \"\"\"Perform a quick hyperparameter test with limited iterations\"\"\"\n",
    "    print(f\"Starting quick hyperparameter test on {dataset_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Extract class names from HuggingFace dataset - ALWAYS get class names for CNN compatibility\n",
    "    class_names = get_class_names_from_dataset(dataset_name)\n",
    "    print(f\"Extracted class names from {dataset_name}: {class_names}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nTesting {model_name}...\")\n",
    "\n",
    "        # Check if model supports hyperparameter tuning\n",
    "        if not hasattr(model, \"get_param_space\"):\n",
    "            print(\n",
    "                f\"[WARNING] {model_name} does not support hyperparameter tuning. Using default params.\"\n",
    "            )\n",
    "            try:\n",
    "                model_copy = deepcopy(model)\n",
    "                # CNN models ALWAYS need class_names (required positional argument)\n",
    "                if \"CNN\" in model_name:\n",
    "                    model_copy.train(X_train, y_train, class_names)\n",
    "                else:\n",
    "                    model_copy.train(X_train, y_train)\n",
    "                metrics = model_copy.evaluate(X_test, y_test)\n",
    "                results[model_name] = {\n",
    "                    \"best_params\": \"default\",\n",
    "                    \"best_score\": metrics.get(\"accuracy\", 0.0),\n",
    "                    \"metrics\": metrics,\n",
    "                }\n",
    "                print(f\"Default accuracy: {metrics.get('accuracy', 0.0):.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {e}\")\n",
    "                results[model_name] = {\"error\": str(e)}\n",
    "            continue\n",
    "\n",
    "        # Get parameter space and sample a few combinations\n",
    "        try:\n",
    "            param_space = model.get_param_space()\n",
    "            param_names = list(param_space.keys())\n",
    "\n",
    "            # Extract actual values from ParamSpace objects - helper function defined in comprehensive section\n",
    "            def extract_param_values_for_quick_test(param_space):\n",
    "                \"\"\"Quick version of param extraction with fewer samples\"\"\"\n",
    "                param_values = []\n",
    "                for param_name, param_def in param_space.items():\n",
    "                    if hasattr(param_def, \"param_type\"):\n",
    "                        # This is a ParamSpace object\n",
    "                        if param_def.param_type.value == \"categorical\":\n",
    "                            param_values.append(param_def.choices)\n",
    "                        elif param_def.param_type.value == \"boolean\":\n",
    "                            param_values.append([True, False])\n",
    "                        elif param_def.param_type.value == \"integer\":\n",
    "                            # Sample a few values from the range for quick testing\n",
    "                            values = []\n",
    "                            if (\n",
    "                                param_def.min_value is not None\n",
    "                                and param_def.max_value is not None\n",
    "                            ):\n",
    "                                step = max(\n",
    "                                    1, (param_def.max_value - param_def.min_value) // 3\n",
    "                                )\n",
    "                                values = list(\n",
    "                                    range(\n",
    "                                        param_def.min_value,\n",
    "                                        param_def.max_value + 1,\n",
    "                                        step,\n",
    "                                    )\n",
    "                                )\n",
    "                                if (\n",
    "                                    len(values) > 5\n",
    "                                ):  # Limit to 5 values for quick testing\n",
    "                                    values = values[:5]\n",
    "                            if (\n",
    "                                param_def.default is not None\n",
    "                                and param_def.default not in values\n",
    "                            ):\n",
    "                                values.append(param_def.default)\n",
    "                            param_values.append(\n",
    "                                values if values else [param_def.default]\n",
    "                            )\n",
    "                        elif param_def.param_type.value == \"float\":\n",
    "                            # Sample a few values from the range for quick testing\n",
    "                            values = []\n",
    "                            if (\n",
    "                                param_def.min_value is not None\n",
    "                                and param_def.max_value is not None\n",
    "                            ):\n",
    "                                import numpy as np\n",
    "\n",
    "                                values = list(\n",
    "                                    np.linspace(\n",
    "                                        param_def.min_value, param_def.max_value, 3\n",
    "                                    )\n",
    "                                )\n",
    "                            if (\n",
    "                                param_def.default is not None\n",
    "                                and param_def.default not in values\n",
    "                            ):\n",
    "                                values.append(param_def.default)\n",
    "                            param_values.append(\n",
    "                                values if values else [param_def.default]\n",
    "                            )\n",
    "                    else:\n",
    "                        # This is already a list of values\n",
    "                        param_values.append(param_def)\n",
    "\n",
    "                return param_values\n",
    "\n",
    "            param_values = extract_param_values_for_quick_test(param_space)\n",
    "\n",
    "            # Generate all combinations and sample a few\n",
    "            all_combinations = list(itertools.product(*param_values))\n",
    "            max_test = min(\n",
    "                3, len(all_combinations)\n",
    "            )  # Test max 3 combinations for speed\n",
    "            test_combinations = (\n",
    "                random.sample(all_combinations, max_test)\n",
    "                if len(all_combinations) > max_test\n",
    "                else all_combinations\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Testing {len(test_combinations)}/{len(all_combinations)} parameter combinations...\"\n",
    "            )\n",
    "\n",
    "            best_score = -1\n",
    "            best_params = None\n",
    "            best_metrics = None\n",
    "\n",
    "            for i, param_combo in enumerate(test_combinations):\n",
    "                current_params = dict(zip(param_names, param_combo))\n",
    "\n",
    "                try:\n",
    "                    # Create fresh model copy\n",
    "                    model_copy = deepcopy(model)\n",
    "\n",
    "                    # Set parameters - handle both sklearn and PyTorch models\n",
    "                    if hasattr(model_copy.model, \"set_params\"):\n",
    "                        # sklearn models\n",
    "                        model_copy.model.set_params(**current_params)\n",
    "                    elif hasattr(model_copy, \"set_params\"):\n",
    "                        # Custom Models like PyTorch\n",
    "                        model_copy.set_params(**current_params)\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Combo {i + 1}: ERROR - Model does not support parameter setting\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    # Train and evaluate - CNN models ALWAYS need class_names\n",
    "                    if \"CNN\" in model_name:\n",
    "                        model_copy.train(X_train, y_train, class_names)\n",
    "                    else:\n",
    "                        model_copy.train(X_train, y_train)\n",
    "                    metrics = model_copy.evaluate(X_test, y_test)\n",
    "                    accuracy = metrics.get(\"accuracy\", 0.0)\n",
    "\n",
    "                    if accuracy > best_score:\n",
    "                        best_score = accuracy\n",
    "                        best_params = current_params.copy()\n",
    "                        best_metrics = metrics.copy()\n",
    "\n",
    "                    print(f\"Combo {i + 1}: accuracy={accuracy:.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Combo {i + 1}: ERROR - {e}\")\n",
    "\n",
    "            results[model_name] = {\n",
    "                \"best_params\": best_params,\n",
    "                \"best_score\": best_score,\n",
    "                \"metrics\": best_metrics,\n",
    "            }\n",
    "\n",
    "            print(f\"Best accuracy: {best_score:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Hyperparameter Tuning failed: {e}\")\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test on MNIST (smaller images, easier for quick demo)\n",
    "print(\"Testing all models on MNIST sample...\")\n",
    "mnist_results = quick_hyperparameter_test(\n",
    "    sample_models, X_mnist, y_mnist, X_mnist_test, y_mnist_test, \"MNIST\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUICK TEST RESULTS SUMMARY - MNIST\")\n",
    "print(\"=\" * 80)\n",
    "for model_name, result in mnist_results.items():\n",
    "    if \"error\" in result:\n",
    "        print(f\"[ERROR] {model_name} failed - {result['error']}\")\n",
    "    else:\n",
    "        print(f\"[RESULT] {model_name}: Accuracy = {result['best_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b263d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on CIFAR-10 as well\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing all models on CIFAR-10 sample...\")\n",
    "cifar_results = quick_hyperparameter_test(\n",
    "    sample_models, X_cifar, y_cifar, X_cifar_test, y_cifar_test, \"CIFAR-10\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUICK TEST RESULTS SUMMARY - CIFAR-10\")\n",
    "print(\"=\" * 80)\n",
    "for model_name, result in cifar_results.items():\n",
    "    if \"error\" in result:\n",
    "        print(f\"[ERROR] {model_name} failed - {result['error']}\")\n",
    "    else:\n",
    "        print(f\"[RESULT] {model_name}: Accuracy = {result['best_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58233334",
   "metadata": {},
   "source": [
    "Noticing the exceptionally low scores, it could be the reason of the **quick** test. It only uses 100 training samples to quickly verify whether the pipeline works normally. So now, we go ahead with a more detailed flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73766f0",
   "metadata": {},
   "source": [
    "## Model Interface Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd785e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all models implement the required interface correctly\n",
    "def verify_model_interface(model_dict):\n",
    "    \"\"\"Verify that all models implement the BaseModel interface correctly\"\"\"\n",
    "    print(\"Verifying model interfaces...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for name, model in model_dict.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "\n",
    "        # Check required methods\n",
    "        required_methods = [\n",
    "            \"create_model\",\n",
    "            \"get_param_space\",\n",
    "            \"train\",\n",
    "            \"predict\",\n",
    "            \"evaluate\",\n",
    "        ]\n",
    "        missing_methods = []\n",
    "\n",
    "        for method in required_methods:\n",
    "            if hasattr(model, method):\n",
    "                print(f\"{method}()\")\n",
    "            else:\n",
    "                print(f\"[WARNING] {method}() - MISSING\")\n",
    "                missing_methods.append(method)\n",
    "\n",
    "        # Check parameter space\n",
    "        try:\n",
    "            if hasattr(model, \"get_param_space\"):\n",
    "                param_space = model.get_param_space()\n",
    "                print(f\"Parameter space: {len(param_space)} parameters\")\n",
    "                for param_name, param_def in param_space.items():\n",
    "                    print(f\"- {param_name}: {param_def.param_type.value}\")\n",
    "            else:\n",
    "                print(\"[WARNING] No parameter space available\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Parameter space error: {e}\")\n",
    "\n",
    "        # Check if model is created\n",
    "        if hasattr(model, \"model\") and model.model is not None:\n",
    "            print(f\"Model instance: {type(model.model).__name__}\")\n",
    "        else:\n",
    "            print(\"No model instance found\")\n",
    "\n",
    "        if missing_methods:\n",
    "            print(f\"[WARNING] INTERFACE INCOMPLETE: Missing {missing_methods}\")\n",
    "        else:\n",
    "            print(\"[RESULT] INTERFACE COMPLETE\")\n",
    "\n",
    "\n",
    "# Run interface verification\n",
    "verify_model_interface(sample_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033742c3",
   "metadata": {},
   "source": [
    "## Quick Demo Summary\n",
    "\n",
    "This quick sample flow demonstrates:\n",
    "\n",
    "1. **All Model Classes Loaded**: Successfully imported and instantiated all 5 model classes from the `models/` directory\n",
    "2. **Interface Compliance**: Verified that all models implement the required `BaseModel` interface\n",
    "3. **Hyperparameter Testing**: Tested hyperparameter tuning functionality with small sample data\n",
    "4. **Training & Evaluation**: Confirmed that all models can train and evaluate on both MNIST and CIFAR-10 data\n",
    "\n",
    "### Models Tested:\n",
    "- ✅ **Decision Tree Model** (`DecisionTreeModel`)\n",
    "- ✅ **K-Nearest Neighbors Model** (`KNNModel`) \n",
    "- ✅ **Logistic Regression Model** (`LogisticRegressionModel`)\n",
    "- ✅ **Multi-Layer Perceptron Model** (`MLPModel`)\n",
    "- ✅ **Convolutional Neural Network Model** (`CNNModel`)\n",
    "\n",
    "This quick flow uses small sample sizes (100 training, 50 test samples) and limited hyperparameter combinations (max 3 per model) to ensure fast execution while still validating that the complete machine learning pipeline works correctly for all implemented model classes.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: For full experiments, use the comprehensive workflow sections below with complete datasets and extensive hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0187a2c7",
   "metadata": {},
   "source": [
    "## Model Interface Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf77f",
   "metadata": {},
   "source": [
    "# Establishing an Ordinary Model Training Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8ecb8",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATASET_DIR = os.path.join(os.path.curdir, \"..\", \".cache\", \"processed_datasets\")\n",
    "CIFAR10_PATH = os.path.join(DATASET_DIR, \"cifar10\")\n",
    "MNIST_PATH = os.path.join(DATASET_DIR, \"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from HuggingFace datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load all datasets from cache\n",
    "cifar10_dataset = load_from_disk(CIFAR10_PATH)\n",
    "mnist_dataset = load_from_disk(MNIST_PATH)\n",
    "\n",
    "# Access train and test splits for all datasets\n",
    "CIFAR10_TRAIN = cifar10_dataset[\"train\"]\n",
    "CIFAR10_TEST = cifar10_dataset[\"test\"]\n",
    "MNIST_TRAIN = mnist_dataset[\"train\"]\n",
    "MNIST_TEST = mnist_dataset[\"test\"]\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Loaded datasets from cache:\")\n",
    "print(f\"CIFAR-10 Train: {len(CIFAR10_TRAIN):,} examples\")\n",
    "print(f\"CIFAR-10 Test: {len(CIFAR10_TEST):,} examples\")\n",
    "print(f\"MNIST Train: {len(MNIST_TRAIN):,} examples\")\n",
    "print(f\"MNIST Test: {len(MNIST_TEST):,} examples\")\n",
    "print(f\"\\nCIFAR-10 classes: {CIFAR10_TRAIN.features['label'].names}\")\n",
    "print(f\"MNIST classes: {list(range(10))}\")  # MNIST has digits 0-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c09ae5",
   "metadata": {},
   "source": [
    "## Prepare a Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b93f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split using HuggingFace datasets train_test_split method\n",
    "cifar10_split = CIFAR10_TRAIN.train_test_split(test_size=500, seed=42)\n",
    "CIFAR10_TRAIN = cifar10_split[\"train\"]\n",
    "CIFAR10_VAL = cifar10_split[\"test\"]\n",
    "\n",
    "mnist_split = MNIST_TRAIN.train_test_split(test_size=500, seed=42)\n",
    "MNIST_TRAIN = mnist_split[\"train\"]\n",
    "MNIST_VAL = mnist_split[\"test\"]\n",
    "\n",
    "# Inspect the sizes\n",
    "print(\"After splitting into Train and Validation sets:\")\n",
    "print(f\"CIFAR-10 Train: {len(CIFAR10_TRAIN):,} examples\")\n",
    "print(f\"CIFAR-10 Validation: {len(CIFAR10_VAL):,} examples\")\n",
    "print(f\"MNIST Train: {len(MNIST_TRAIN):,} examples\")\n",
    "print(f\"MNIST Validation: {len(MNIST_VAL):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed17280",
   "metadata": {},
   "source": [
    "## Get the Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.curdir, \"..\"))\n",
    "\n",
    "\n",
    "# Instantiate model classes (not the sklearn models directly)\n",
    "dt_model = DecisionTreeModel()\n",
    "dt_model.create_model()\n",
    "knn_model = KNNModel()\n",
    "knn_model.create_model()\n",
    "lr_model = LogisticRegressionModel()\n",
    "lr_model.create_model()\n",
    "mlp_model = MLPModel()\n",
    "mlp_model.create_model()\n",
    "cnn_model = CNNModel()\n",
    "cnn_model.create_model()\n",
    "\n",
    "# Display those model instances\n",
    "models = {\n",
    "    \"Decision Tree Model\": dt_model,\n",
    "    \"K-Nearest Neighbors Model\": knn_model,\n",
    "    \"Logistic Regression Model\": lr_model,\n",
    "    \"Multi-Layer Perceptron Model\": mlp_model,\n",
    "    \"Convolutional Neural Network Model\": cnn_model,\n",
    "}\n",
    "\n",
    "# Show the model classes\n",
    "for name, model in models.items():\n",
    "    print(f\"{name}: {type(model).__name__}\")\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d878aa",
   "metadata": {},
   "source": [
    "## Tune the Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060068c",
   "metadata": {},
   "source": [
    "We tune it with our own multi-objective fitness functions across different metaheuristics. For simlicity, let's just demonstrate how we weigh the importances of every metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6da5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: extract_features_and_labels is defined in the quick demo section above with max_samples parameter. We are reusing it here.\n",
    "\n",
    "# Extract validation features and labels for CIFAR-10\n",
    "X_CIFAR10_VAL, y_CIFAR10_VAL = extract_features_and_labels(\n",
    "    CIFAR10_VAL, flatten_images=True\n",
    ")\n",
    "print(\"CIFAR-10 Validation:\")\n",
    "print(f\"X_VAL shape: {X_CIFAR10_VAL.shape}\")\n",
    "print(f\"y_VAL shape: {y_CIFAR10_VAL.shape}\")\n",
    "print(f\"Data type: X={X_CIFAR10_VAL.dtype}, y={y_CIFAR10_VAL.dtype}\")\n",
    "\n",
    "# Extract validation features and labels for MNIST\n",
    "X_MNIST_VAL, y_MNIST_VAL = extract_features_and_labels(MNIST_VAL, flatten_images=True)\n",
    "print(\"\\nMNIST Validation:\")\n",
    "print(f\"X_VAL shape: {X_MNIST_VAL.shape}\")\n",
    "print(f\"y_VAL shape: {y_MNIST_VAL.shape}\")\n",
    "print(f\"Data type: X={X_MNIST_VAL.dtype}, y={y_MNIST_VAL.dtype}\")\n",
    "\n",
    "# Show label examples\n",
    "print(f\"\\nCIFAR-10 label examples: {y_CIFAR10_VAL[:5]}\")\n",
    "print(f\"MNIST label examples: {y_MNIST_VAL[:5]}\")\n",
    "print(f\"Pixel value range CIFAR-10: [{X_CIFAR10_VAL.min()}, {X_CIFAR10_VAL.max()}]\")\n",
    "print(f\"Pixel value range MNIST: [{X_MNIST_VAL.min()}, {X_MNIST_VAL.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc45f5",
   "metadata": {},
   "source": [
    "The cell below shows 3 alternative methods of extracting the images array and the labels array, just as a reference to check whether the above extraction behaves correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5366929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative methods for extracting X and y\n",
    "\n",
    "# Method 1: Direct column access (simpler but less flexible)\n",
    "print(\"=== Alternative Method 1: Direct Column Access ===\")\n",
    "y_cifar_simple = np.array(CIFAR10_VAL[\"label\"])\n",
    "print(f\"y_CIFAR10_VAL shape: {y_cifar_simple.shape}\")\n",
    "\n",
    "# Method 2: Using dataset.to_pandas()\n",
    "print(\"\\n=== Alternative Method 2: Using to_pandas() ===\")\n",
    "cifar_df = CIFAR10_VAL.to_pandas()\n",
    "print(f\"DataFrame shape: {cifar_df.shape}\")\n",
    "print(f\"DataFrame columns: {list(cifar_df.columns)}\")\n",
    "\n",
    "# Method 3: Batch processing for large datasets (memory efficient)\n",
    "print(\"\\n=== Alternative Method 3: Batch Processing ===\")\n",
    "\n",
    "\n",
    "def extract_in_batches(dataset, batch_size=1000, flatten_images=True):\n",
    "    \"\"\"Extract features and labels in batches to save memory\"\"\"\n",
    "    total_samples = len(dataset)\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch = dataset[i : i + batch_size]\n",
    "\n",
    "        # Process batch\n",
    "        batch_images = []\n",
    "        for img in batch[\"image\"]:\n",
    "            img_array = np.array(img)\n",
    "            if flatten_images:\n",
    "                img_array = img_array.flatten()\n",
    "            batch_images.append(img_array)\n",
    "\n",
    "        X_batches.append(np.array(batch_images))\n",
    "        y_batches.append(np.array(batch[\"label\"]))\n",
    "\n",
    "    # Concatenate all batches\n",
    "    X = np.vstack(X_batches)\n",
    "    y = np.concatenate(y_batches)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Example with small batch for demonstration\n",
    "X_batch, y_batch = extract_in_batches(CIFAR10_VAL.select(range(100)), batch_size=50)\n",
    "print(f\"Batch extraction example - X shape: {X_batch.shape}, y shape: {y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d12bc",
   "metadata": {},
   "source": [
    "### Tuning the Hyperparameters by training on only **validation set** iteratively on each dataset and each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aec65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def calculate_weighted_score(metrics_dict):\n",
    "    \"\"\"Calculate weighted sum of evaluation metrics\"\"\"\n",
    "    weighted_sum = 0.0\n",
    "    for metric_name, score in metrics_dict.items():\n",
    "        if metric_name in metrics_dict:\n",
    "            weighted_sum += (\n",
    "                score * 1 / len(metrics_dict)\n",
    "            )  # Equal weights for simplicity\n",
    "    weighted_sum *= 0.8  # avoid going to 1\n",
    "    return weighted_sum\n",
    "\n",
    "\n",
    "def get_class_names_from_dataset(dataset_name):\n",
    "    \"\"\"Extract class names from HuggingFace dataset features with reliable fallbacks\"\"\"\n",
    "    try:\n",
    "        if dataset_name == \"MNIST\":\n",
    "            # Try to get from MNIST dataset features\n",
    "            if (\n",
    "                hasattr(MNIST_TRAIN.features[\"label\"], \"names\")\n",
    "                and MNIST_TRAIN.features[\"label\"].names\n",
    "            ):\n",
    "                return MNIST_TRAIN.features[\"label\"].names\n",
    "            else:\n",
    "                # MNIST typically uses digits 0-9\n",
    "                return [str(i) for i in range(10)]\n",
    "        elif dataset_name == \"CIFAR-10\":\n",
    "            # Try to get from CIFAR-10 dataset features\n",
    "            if (\n",
    "                hasattr(CIFAR10_TRAIN.features[\"label\"], \"names\")\n",
    "                and CIFAR10_TRAIN.features[\"label\"].names\n",
    "            ):\n",
    "                return CIFAR10_TRAIN.features[\"label\"].names\n",
    "            else:\n",
    "                # CIFAR-10 fallback class names\n",
    "                return [\n",
    "                    \"airplane\",\n",
    "                    \"automobile\",\n",
    "                    \"bird\",\n",
    "                    \"cat\",\n",
    "                    \"deer\",\n",
    "                    \"dog\",\n",
    "                    \"frog\",\n",
    "                    \"horse\",\n",
    "                    \"ship\",\n",
    "                    \"truck\",\n",
    "                ]\n",
    "        else:\n",
    "            # For other datasets, create generic class names\n",
    "            return [f\"Class {i}\" for i in range(10)]  # Default to 10 classes\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not extract class names for {dataset_name}: {e}\")\n",
    "        # Reliable fallbacks\n",
    "        if dataset_name == \"MNIST\":\n",
    "            return [str(i) for i in range(10)]\n",
    "        elif dataset_name == \"CIFAR-10\":\n",
    "            return [\n",
    "                \"airplane\",\n",
    "                \"automobile\",\n",
    "                \"bird\",\n",
    "                \"cat\",\n",
    "                \"deer\",\n",
    "                \"dog\",\n",
    "                \"frog\",\n",
    "                \"horse\",\n",
    "                \"ship\",\n",
    "                \"truck\",\n",
    "            ]\n",
    "        else:\n",
    "            return [f\"Class {i}\" for i in range(10)]\n",
    "\n",
    "\n",
    "# Since ParamSpace is a custom class, we need to extract actual values with a helper function\n",
    "def extract_param_values_from_param_space(param_space):\n",
    "    \"\"\"Extract actual values from ParamSpace objects for hyperparameter tuning\"\"\"\n",
    "    param_values = []\n",
    "    for param_name, param_def in param_space.items():\n",
    "        if hasattr(param_def, \"param_type\"):\n",
    "            # This is a ParamSpace object\n",
    "            if param_def.param_type.value == \"categorical\":\n",
    "                param_values.append(param_def.choices)\n",
    "            elif param_def.param_type.value == \"boolean\":\n",
    "                param_values.append([True, False])\n",
    "            elif param_def.param_type.value == \"integer\":\n",
    "                # Sample a few values from the range for comprehensive testing\n",
    "                values = []\n",
    "                if param_def.min_value is not None and param_def.max_value is not None:\n",
    "                    step = max(1, (param_def.max_value - param_def.min_value) // 5)\n",
    "                    values = list(\n",
    "                        range(param_def.min_value, param_def.max_value + 1, step)\n",
    "                    )\n",
    "                    if len(values) > 8:  # Limit to 8 values for comprehensive testing\n",
    "                        values = values[:8]\n",
    "                if param_def.default is not None and param_def.default not in values:\n",
    "                    values.append(param_def.default)\n",
    "                param_values.append(values if values else [param_def.default])\n",
    "            elif param_def.param_type.value == \"float\":\n",
    "                # Sample a few values from the range for comprehensive testing\n",
    "                values = []\n",
    "                if param_def.min_value is not None and param_def.max_value is not None:\n",
    "                    import numpy as np\n",
    "\n",
    "                    values = list(\n",
    "                        np.linspace(param_def.min_value, param_def.max_value, 5)\n",
    "                    )\n",
    "                if param_def.default is not None and param_def.default not in values:\n",
    "                    values.append(param_def.default)\n",
    "                param_values.append(values if values else [param_def.default])\n",
    "        else:\n",
    "            # This is already a list of values\n",
    "            param_values.append(param_def)\n",
    "\n",
    "    return param_values\n",
    "\n",
    "\n",
    "def _sanitize_paths_in_obj(obj, start=None):\n",
    "    \"\"\"Recursively convert absolute filesystem paths in strings to relative paths for safe printing.\"\"\"\n",
    "    if start is None:\n",
    "        try:\n",
    "            start = os.getcwd()\n",
    "        except Exception:\n",
    "            start = \".\"\n",
    "\n",
    "    # Strings: convert absolute paths to relative where possible\n",
    "    if isinstance(obj, str):\n",
    "        try:\n",
    "            if os.path.isabs(obj):\n",
    "                return os.path.relpath(obj, start)\n",
    "        except Exception:\n",
    "            return obj\n",
    "        return obj\n",
    "\n",
    "    # Dicts: recurse\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _sanitize_paths_in_obj(v, start) for k, v in obj.items()}\n",
    "\n",
    "    # Lists / tuples: recurse and preserve type\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        converted = [_sanitize_paths_in_obj(v, start) for v in obj]\n",
    "        return type(obj)(converted)\n",
    "\n",
    "    # Other types: leave as-is\n",
    "    return obj\n",
    "\n",
    "\n",
    "# Storage for hyperparameter tuning results\n",
    "tuning_results = {}\n",
    "model_dataset_param = {}  # To Store \"Best\" Models\n",
    "\n",
    "for dataset in [\"CIFAR-10\", \"MNIST\"]:\n",
    "    for modelName, model in models.items():\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Tuning hyperparameters for {modelName} on {dataset}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "        # Get class names for this dataset - ALWAYS get them for CNN compatibility\n",
    "        class_names = get_class_names_from_dataset(dataset)\n",
    "        print(f\"Using class names: {class_names}\")\n",
    "\n",
    "        # Get the appropriate validation data\n",
    "        match dataset:\n",
    "            case \"CIFAR-10\":\n",
    "                X_val, y_val = X_CIFAR10_VAL, y_CIFAR10_VAL\n",
    "            case \"MNIST\":\n",
    "                X_val, y_val = X_MNIST_VAL, y_MNIST_VAL\n",
    "\n",
    "        # Get parameter space\n",
    "        if not hasattr(model, \"get_param_space\"):\n",
    "            print(\n",
    "                f\"[WARNING] Model {modelName} does not support hyperparameter tuning. Using default params.\"\n",
    "            )\n",
    "            try:\n",
    "                model_copy = deepcopy(model)\n",
    "                # CNN models ALWAYS need class_names (required positional argument)\n",
    "                if \"CNN\" in modelName:\n",
    "                    model_copy.train(X_val, y_val, class_names)\n",
    "                else:\n",
    "                    model_copy.train(X_val, y_val)\n",
    "                eval_metrics = model_copy.evaluate(X_val, y_val)\n",
    "                weighted_score = calculate_weighted_score(eval_metrics)\n",
    "\n",
    "                # Store results for models that don't support hyperparameter tuning\n",
    "                tuning_results[(modelName, dataset)] = {\n",
    "                    \"best_params\": \"default\",\n",
    "                    \"best_score\": weighted_score,\n",
    "                    \"best_metrics\": eval_metrics,\n",
    "                    \"all_results\": [\n",
    "                        {\n",
    "                            \"params\": \"default\",\n",
    "                            \"metrics\": eval_metrics,\n",
    "                            \"weighted_score\": weighted_score,\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "                model_dataset_param[(modelName, dataset)] = model_copy\n",
    "                print(f\"Default weighted score: {weighted_score:.4f}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Error with default params: {e}\")\n",
    "                tuning_results[(modelName, dataset)] = {\"error\": str(e)}\n",
    "                model_dataset_param[(modelName, dataset)] = None\n",
    "                continue\n",
    "\n",
    "        param_space = model.get_param_space()\n",
    "        param_names = list(param_space.keys())\n",
    "\n",
    "        # Extract actual values from ParamSpace objects\n",
    "        param_values = extract_param_values_from_param_space(param_space)\n",
    "\n",
    "        # Generate all combinations of parameters\n",
    "        param_combinations = list(itertools.product(*param_values))\n",
    "        # Shuffle to randomize order because of iteration limit\n",
    "        # Make sure every combination have an equal chance to be explored\n",
    "        random.shuffle(param_combinations)\n",
    "        total_combinations = len(param_combinations)\n",
    "        print(f\"Testing {total_combinations} parameter combinations...\")\n",
    "\n",
    "        best_score = -1\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        current_results = []\n",
    "\n",
    "        # Iterate through all parameter combinations\n",
    "        time_start = time.perf_counter()\n",
    "        time_limit = 60 * 10  # 10 minutes\n",
    "        iteration_count = 0\n",
    "        iteration_limit = total_combinations * 0.6\n",
    "        for i, param_combo in enumerate(param_combinations):\n",
    "            # Check the Limits\n",
    "            if time.perf_counter() - time_start > time_limit:\n",
    "                print(\"Time limit reached, stopping further evaluations.\")\n",
    "                break\n",
    "            if iteration_count >= iteration_limit:\n",
    "                print(\"Iteration limit reached, stopping further evaluations.\")\n",
    "                break\n",
    "            # Create parameter dictionary\n",
    "            current_params = dict(zip(param_names, param_combo))\n",
    "\n",
    "            # Create a fresh copy of the model for this configuration\n",
    "            model_copy = deepcopy(model)\n",
    "\n",
    "            # Set parameters - handle both sklearn and PyTorch models\n",
    "            try:\n",
    "                if hasattr(model_copy.model, \"set_params\"):\n",
    "                    # sklearn models\n",
    "                    model_copy.model.set_params(**current_params)\n",
    "                elif hasattr(model_copy, \"set_params\"):\n",
    "                    # Custom Models like PyTorch\n",
    "                    model_copy.set_params(**current_params)\n",
    "                else:\n",
    "                    safe_params = _sanitize_paths_in_obj(current_params)\n",
    "                    print(\n",
    "                        f\"Error with params {safe_params}: Model does not support parameter setting\"\n",
    "                    )\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                safe_params = _sanitize_paths_in_obj(current_params)\n",
    "                print(f\"Error setting params {safe_params}: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Train the model - CNN models ALWAYS need class_names (required positional argument)\n",
    "                if \"CNN\" in modelName:\n",
    "                    model_copy.train(X_val, y_val, class_names)\n",
    "                else:\n",
    "                    model_copy.train(X_val, y_val)\n",
    "\n",
    "                # Evaluate the model\n",
    "                eval_metrics = model_copy.evaluate(X_val, y_val)\n",
    "\n",
    "                # Calculate weighted score\n",
    "                weighted_score = calculate_weighted_score(eval_metrics)\n",
    "\n",
    "                # Store results\n",
    "                result_entry = {\n",
    "                    \"params\": current_params.copy(),\n",
    "                    \"metrics\": eval_metrics.copy(),\n",
    "                    \"weighted_score\": weighted_score,\n",
    "                }\n",
    "                current_results.append(result_entry)\n",
    "\n",
    "                # Update best model if this one is better\n",
    "                if weighted_score > best_score:\n",
    "                    best_score = weighted_score\n",
    "                    best_params = current_params.copy()\n",
    "                    best_model = deepcopy(model_copy)\n",
    "\n",
    "                # Progress indicator\n",
    "                if (i + 1) % max(\n",
    "                    1, total_combinations // 10\n",
    "                ) == 0 or i == total_combinations - 1:\n",
    "                    print(\n",
    "                        f\"Progress: {i + 1}/{total_combinations} ({(i + 1) / total_combinations * 100:.1f}%) - \"\n",
    "                        f\"Current best weighted score: {best_score:.4f}\"\n",
    "                    )\n",
    "\n",
    "                # PERFORMANCE: Halt the process when it reaches near-perfect score\n",
    "                # Avoids Overfitting!\n",
    "                if best_score >= 0.93 * 0.8:  # applied penalty to scores\n",
    "                    print(\n",
    "                        \"Reached excellent score (≥0.93), stopping further evaluations.\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                # Sanitize any path-like values in current_params before printing\n",
    "                safe_params = _sanitize_paths_in_obj(current_params)\n",
    "                print(f\"Error with params {safe_params}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            iteration_count += 1\n",
    "\n",
    "        # Store results for this model-dataset combination\n",
    "        tuning_results[(modelName, dataset)] = {\n",
    "            \"best_params\": best_params,\n",
    "            \"best_score\": best_score,\n",
    "            \"best_metrics\": best_model.evaluate(X_val, y_val) if best_model else None,\n",
    "            \"all_results\": current_results,\n",
    "        }\n",
    "\n",
    "        # Store the best trained model\n",
    "        model_dataset_param[(modelName, dataset)] = best_model\n",
    "\n",
    "        # Print summary for this model-dataset combination\n",
    "        print(f\"\\nBest configuration for {modelName} on {dataset}:\")\n",
    "        print(f\"Parameters: {_sanitize_paths_in_obj(best_params)}\")\n",
    "        print(f\"Weighted Score: {best_score:.4f}\")\n",
    "        if best_model:\n",
    "            best_metrics = best_model.evaluate(X_val, y_val)\n",
    "            for metric, value in best_metrics.items():\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"HYPERPARAMETER TUNING COMPLETED!\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(\"Results summary:\")\n",
    "for (model_name, dataset_name), results in tuning_results.items():\n",
    "    if \"error\" in results:\n",
    "        print(f\"{model_name} on {dataset_name}: [ERROR] {results['error']}\")\n",
    "    else:\n",
    "        safe_best_params = _sanitize_paths_in_obj(results.get(\"best_params\"))\n",
    "        print(\n",
    "            f\"{model_name} on {dataset_name}: Best weighted score = {results['best_score']:.4f}; params={safe_best_params}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25822687",
   "metadata": {},
   "source": [
    "The tuning might be too naive since it just kept tuning till reaching the best metrics (although it already caps at 0.93). That case must be overfitting (i.e., memorizing) solely the training (seen) data. We need more sophisticated logic. But now at least we know our end-to-end flow works well during the pure tuning processes before introducing metaheuristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cacf7be",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a17d96",
   "metadata": {},
   "source": [
    "Train it with best set of parameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0502d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_best_models: dict = {}\n",
    "for (modelName, datasetName), results in tuning_results.items():\n",
    "    print(\n",
    "        f\"\\nTraining final model for {modelName} on {datasetName} with best hyperparameters...\"\n",
    "    )\n",
    "\n",
    "    # Check if tuning was successful\n",
    "    if \"error\" in results:\n",
    "        print(\n",
    "            f\"[ERROR] Skipping {modelName} on {datasetName} due to tuning error: {results['error']}\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    best_model = model_dataset_param.get((modelName, datasetName))\n",
    "    if best_model is None:\n",
    "        print(\n",
    "            f\"[ERROR] No best model found for {modelName} on {datasetName}, skipping.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Get class names for CNN models - ALWAYS get them for CNN compatibility\n",
    "    class_names = get_class_names_from_dataset(datasetName)\n",
    "\n",
    "    # Retrain on full training data (train only)\n",
    "    match datasetName:\n",
    "        case \"CIFAR-10\":\n",
    "            X_train, y_train = extract_features_and_labels(\n",
    "                CIFAR10_TRAIN, flatten_images=True\n",
    "            )\n",
    "        case \"MNIST\":\n",
    "            X_train, y_train = extract_features_and_labels(\n",
    "                MNIST_TRAIN, flatten_images=True\n",
    "            )\n",
    "    if not hasattr(best_model, \"train\"):\n",
    "        print(\n",
    "            f\"[WARNING] Model {modelName} does not support training method. Skipping.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Train - CNN models ALWAYS need class_names (required positional argument)\n",
    "    if \"CNN\" in modelName:\n",
    "        best_model.train(X_train, y_train, class_names)\n",
    "    else:\n",
    "        best_model.train(X_train, y_train)\n",
    "    print(\"Training completed.\")\n",
    "    # Save Trained Models\n",
    "    trained_best_models[(modelName, datasetName)] = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e685914",
   "metadata": {},
   "source": [
    "## Evaluate the Trained Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics: dict = {}\n",
    "for (modelName, datasetName), best_model in trained_best_models.items():\n",
    "    print(f\"\\nEvaluating final trained model for {modelName} on {datasetName}...\")\n",
    "    # Get test data\n",
    "    match datasetName:\n",
    "        case \"CIFAR-10\":\n",
    "            X_test, y_test = extract_features_and_labels(\n",
    "                CIFAR10_TEST, flatten_images=True\n",
    "            )\n",
    "        case \"MNIST\":\n",
    "            X_test, y_test = extract_features_and_labels(\n",
    "                MNIST_TEST, flatten_images=True\n",
    "            )\n",
    "    # Evaluate\n",
    "    test_metrics = best_model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Metrics for {modelName} on {datasetName}:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "        # Save metrics\n",
    "    metrics[(modelName, datasetName)] = test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Best Model from Weighted Sum\n",
    "best_overall_score = -1\n",
    "for (modelName, datasetName), test_metrics in metrics.items():\n",
    "    weighted_score = calculate_weighted_score(test_metrics)\n",
    "    print(f\"Weighted Score for {modelName} on {datasetName}: {weighted_score:.4f}\")\n",
    "    if weighted_score > best_overall_score:\n",
    "        best_overall_score = weighted_score\n",
    "        best_model_info = (modelName, datasetName)\n",
    "print(\n",
    "    f\"\\nBest Overall Model: {best_model_info[0]} on {best_model_info[1]} with Weighted Score: {best_overall_score:.4f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
